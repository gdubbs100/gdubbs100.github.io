---
overview: "A guide on how MPC works without getting bogged down in dynamics models"
---
# Model Predictive Control with black-box dynamics models

I've always found decision-making agents one of the most interesting areas of Artificial Intelligence (AI). It was this fascination which drew me to studying Reinforcement Learning (RL) in my master's thesis (link?). However, my constant frustration with playing with RL algorithms is how long it takes to get an agent to learn anything. When running models for my thesis, despite using a high performance computing cluster with A100 GPUs my quicker training runs would take 6-7 hours, and for some more complicated models I was waiting several days to get results.

After finishing my studies this issue only got worse. My personal computer is a laptop without a GPU, so training RL agents on anything but the simplest environments takes several hours. Its not just GPU and compute that causes this, RL is notoriously sample inefficient and brittle [source??]. This frustration led me to start looking into alternative ways to build decision making agents that would not require extensive time and compute. Yann LeCunn is frequently quoted as stating<a href="https://drive.google.com/file/d/1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi/view"> "Abandon Reinforcement Learning in favour of Model Predictive Control"</a>, the reasoning being that if you have a world / dynamics model, then you can use MPC to plan a series of actions without the lengthy training time required by model-free RL. [but of course - where do you get a world model??]

## Figuring out Model Predictive Control
My initial attempts at learning MPC were actually ill fated. I found that introductions were either trivial or overly focussed on specific applications. In the first case, only the general skeleton of the algorithm was described - which is easy enough to understand, but leaves important gaps regarding how to actually _implement_ the thing. In the second, use-case specific dynamics models were described in great detail, and this would distract from how the MPC algorithm _actually_ worked. MPC is a technique often used in engineering fields, so I found many descriptions often assumed a significant working knowledge of physics and differential equations. 

### MPC at a high-level
MPC is a relatively simple algorithm that involves taking an action and predicting the reward (or cost) of that action by simulating the outcome of the action using a model of your environment. The typical approach is to select actions for k-steps into the future (a trajectory), and evaluate the total reward of that trajectory. You run several of these rollouts and pick the trajectory that achieves the best reward or lowest cost and then take the first action from that trajectory to apply in your environment. You repeat this everytime you need to make a decision, thus enabling you to choose actions which should maximise your reward over a forward window based on your knowledge of the environment.

$$
\begin{array}{l}
\hline
\textbf{Algorithm: } \text{Model Predictive Control (MPC)} \\
\hline
\textbf{Input: } \text{Initial State } x_0, \text{ Rollout length } k, \text{ Number of rollouts } n, \text{ Model fn } f_\theta, \text{ Reward fn } R \\
\textbf{Output: } \text{First action } u_t^* \\
\hline
1: \text{Generate } n \text{ sequences: } \{ \mathbf{u}^{(i)}_{0:k-1} \}_{i=1}^n \\
2: \textbf{for } \text{each sequence } i = 1 \dots n \textbf{ do} \\
3: \quad \text{Predict trajectory using } \hat{x}_{j+1} = f_\theta(\hat{x}_j, u_j) \\
4: \quad \text{Calculate total reward: } J^{(i)} = \sum_{j=0}^{k-1} R(\hat{x}_j, u_j) \\
5: \textbf{end for} \\
6: \text{Calculate the best action } u^{*} \text{ from: } \mathbf{J} \\
7: \textbf{return } u^{*} \\
\hline
\end{array}
$$

So far so good - but we need to think a bit more before we can actually implement this algorithm:

1. How do you generate trajectories of actions?
2. How do you select the best action?
3. Where and how do you get the dynamics model?

Firstly, on point 3 - while a dynamics model is an essential component of MPC, I'm going to actually ignore this part. The reason being is that the dynamics model is an input to MPC, so we can take it as given for now. My experience in trying to learn about MPC is that many articles spend a lot of time discussing and detailing the dynamics model - which can be very complex in many engineering applications - and this gets in the way of understanding the MPC algorithm itself. For most of this article, we will treat the dynamics model as a black-box.

MPC algorithms answer questions 1 and 2 above. They are essentially designed to generate a set of actions, and then use the outputs of the dynamics and reward functions to select the next action. These two tasks are essentially solving an optimisation problem: find the actions that maximise the reward (or minimise the cost) over the next k-steps in the future. I'm now going to go into details on 4 of these algorithms and then provide a brief demonstration of how they work on a toy mountain-cart style environment.

### 1. Random Shooting
The simplest approach is to generate actions by random sampling. We select a rollout length of $k$, a number of rollouts $n$, and then randomly sample $k \times n$ actions and pass each of these through our dynamics model. Take the action trajectory which achieved the best reward and take the first action from that trajectory to be applied in the environment.

$$
\begin{array}{l}
\hline
\textbf{Algorithm: } \text{Random Shooting MPC} \\
\hline
\textbf{Input: } \text{Initial State } x_0, \text{ Rollout length } k, \text{ Number of rollouts } n, \text{ Model fn } f_\theta, \text{ Reward fn } R \\
\textbf{Output: } \text{First action } u_t^* \\
\hline\
1: \text{Sample } n \text{ random sequences: } \{ \mathbf{u}^{(i)}_{0:k-1} \}_{i=1}^n \\
2: \textbf{for } \text{each sequence } i = 1 \dots n \textbf{ do} \\
3: \quad \text{Predict trajectory using } \hat{x}_{j+1} = f_\theta(\hat{x}_j, u_j) \\
4: \quad \text{Calculate total reward: } J^{(i)} = \sum_{j=0}^{k-1} R(\hat{x}_j, u_j) \\
5: \textbf{end for} \\
6: \text{Select best sequence: } i^* = \arg\max_i J^{(i)} \\
7: \textbf{return } u_t^* = \mathbf{u}^{(i^*)}_{0} \\
\hline
\end{array}
$$

Random shooting is simple to implement and quick to run, however, you might wonder how well you can select the correct actions simply by random sampling. In addition, there is no learning or refinement of actions. In that respect, random shooting is inefficient in generating good actions as the only rollout that determines your reward is the one you select - but you ignore all the other rollouts. This is suboptimal because those rollouts contain valuable information about how selected actions can affect rewards.

### 2. The Cross-Entropy-Method (CEM)
CEM is a genetic algorithm or population-based method, which randomly samples actions and then refines those actions by aggregating the top-m or top-$\alpha$ percentile reward-achieving-trajectories. The algorithm works much like the random-shooting algorithm with the addition of the refinement process. 

As a concrete example, assume we sample actions from a standard (multivariate) normal distribution: $ \mathbf{a}\_{t} \sim N(\mathbf{0}\_{n}, \mathbf{I}\_{n}) $. We then run  trajectories through the dynamics model and rank them by achieved rewards. We select the $m$ best, or use a percentile cutoff, to create a subset of trajectories referred to as "elites". Using only these elites we recalculate the entries of the mean and standard deviation of the sampling distribution from the corresponding vector position of the elites. That is, we maintain and update a normal distribution for each step of the rollout. After a $I$ learning iterations, we can sample an action from the first distribution or just take the mean.

$$
\begin{array}{l}
\hline
\textbf{Algorithm: } \text{Cross-Entropy Method (CEM)} \\
\hline
\textbf{Input: } \text{Initial State } x_0, \text{ Rollout length } k, \text{ Number of Rollouts } n, \text{ Number of Elites } m, \text{ Iterations } I \\ 
\text{ Model fn } f_\theta, \text{ Reward fn } R \\
\textbf{Output: } \text{First action } u_t^* \\
\textbf{Initialize: } \mu_{j} = \mathbf{0}_n, \sigma_{j} = \mathbf{I}_n \text{ for } j = 0 \dots k-1 \\
\hline
1: \textbf{for } \text{iteration } 1 \dots I \textbf{ do} \\
2: \quad \text{Sample } n \text{ sequences: } \mathbf{u}^{(i)}_{0:k-1} \sim \mathcal{N}(\mu_{0:k-1}, \sigma_{0:k-1}) \\
3: \quad \textbf{for } \text{each sequence } i = 1 \dots n \textbf{ do} \\
4: \quad \quad \text{Predict trajectory } \hat{x}_{j+1} = f_\theta(\hat{x}_j, u_j) \\
5: \quad \quad \text{Calculate total reward: } J^{(i)} = \sum_{j=0}^{k-1} R(\hat{x}_j, u_j) \\
6: \quad \textbf{end for} \\
7: \quad \text{Select top-} m \text{  sequences with highest } J^{(i)} \\
8: \quad \text{Update } \mu_j = \text{mean}(\text{elites}_j) \text{ for } j = 0 \dots k-1 \\
9: \quad \text{Update } \sigma_j = \text{std}(\text{elites}_j) \text{ for } j = 0 \dots k-1 \\
10: \textbf{end for} \\
11: \textbf{return } u_t^* = \mu_0 \\
\hline
\end{array}
$$

One benefit of the CEM algorithm is that it is completely agnostic of the structure of the problem to which it is applied. It is a general purpose optimisation algorithm which can be applied in many domains [cite paper]. However, this is also a weakness as it means its sampling and updates of actions do not learn from the structure of the environment - this is a missed opportunity and can lead to slower optimisation. CEM reportedly also struggles with high dimensional action spaces. Despite this, it is a flexible algorithm and is relatively simple to implement.

### 3. Model Predictive Path Integral (MPPI)
MPPI is another population based method which is based around the concept of an 'optimal information theoretic control law' [cite]. I'll be honest here - I don't fully understand what this means and why it is good. I think the high level view is that the algorithm assumes and incorporates an 'optimal' amount of randomness to let it explore new actions while achieving good performance. 

The MPPI algorithm assumes that an action isn't directly applied in the environment. Rather there is a stochastic relationship between the action intended and the action that actually ends up being applied in the system, given by $v_{t} \sim N(u_{t}, \Sigma)$. The algorithm starts by sampling noise $\epsilon_{t} \sim N(0, 1)$ and combining this with a base set of actions $v_{t} = u_{t} + \epsilon_{t}$ and collecting rewards via dynamics model simulation. Once collecting the rewards, MPPI updates the actions $u_{t}$ using a weighted average of the rewards:

$$
u_{t}^{(i)} = u_{t}^{(i-1)} + \sum^{N}_{n=1}{w}\epsilon_{n}^{(i-1)}
$$

Where the weighting is given by:

$$
w = \frac{1}{Z}\exp({-\frac{1}{\lambda}(J^{(k)} - \text{min}(J))})
$$

and $Z$ is a normalising constant and $\lambda$ is a temperature parameter. Larger values of $\lambda$ result in a closer to uniform weighting. 

$$
\begin{array}{l}
\hline
\textbf{Algorithm: } \text{Model Predictive Path Integral (MPPI)} \\
\hline
\textbf{Input: } \text{State } x_t, \text{ Horizon } k, \text{ Samples } N, \text{ Temperature } \lambda, \text{ Noise } \Sigma \\
\textbf{Initialize: } \text{Mean action sequence } \mathbf{u}_{0:k-1} \\
\hline
1: \textbf{Sample } N \text{ noise sequences: } \epsilon^{(n)}_{0:k-1} \sim \mathcal{N}(0, \Sigma) \\
2: \textbf{for } \text{each sample } n = 1 \dots N \textbf{ do} \\
3: \quad \text{Perturb actions: } v^{(n)}_j = u_j + \epsilon^{(n)}_j \\
4: \quad \text{Roll out trajectory: } \hat{x}_{j+1} = f_\theta(\hat{x}_j, v^{(n)}_j) \\
5: \quad \text{Calculate total cost/reward: } J^{(n)} \\
6: \textbf{end for} \\
7: \text{Compute weights: } w^{(n)} = \frac{1}{Z} \exp \left( \frac{1}{\lambda} (J^{(n)} - \max(J)) \right) \\
8: \text{Update mean sequence: } u_j \leftarrow u_j + \sum_{n=1}^{N} w^{(n)} \epsilon^{(n)}_j \\
9: \textbf{return } u_t^* = u_0 \\
\hline
\end{array}
$$

The strength of MPPI is that it uses _all_ of the sample paths generated in the rollouts.

### 4. Gradient-Based (GB) MPC
In contrast to population based methods, GB MPC directly updates the action trajectories using gradient descent. Doing this requires that we are able to differentiate the dynamics model - which is a bit of a deviation from the black box assumption we've strived for in this article. Essentially:

$$ 
a^{(i)}_{t} = a^{(i-1)}_{t} + \eta \nabla{J}
$$

Where the reward (or cost) equation $J$ is given by something like: 

$$
J=-\Sigma^{n}_{i = t+1}{\hat{x}_{i}R\hat{x}_{i}^{T} + u_{i}Qu_{i}^{T}}
$$ 

We can see that $J$ depends on the estimates of the state $\hat{x_{t}}$ which is given by:

$$
\hat{x}_{t+1} = f(x_{t}, u_{t})
$$

This means that in order to differentiate $J$ with respect to $u_{t}$ we need to differentiate the dynamics model! urgh...

Fortunately, due to the presence of auto-diff enabled libraries like torch, tensorflow and jax, if we write our environment (or someone else does) in a certain manner we can run this algorithm without needing to manually derive the gradients of the environment, and we can still treat it like a black box. Note that (to reiterate) - we always need some sort of dynamics model to run MPC, but I'm ignoring it for the purposes of describing the essence of the MPC algorithms clearly. Note also - if you learnt your model of dynamics using a neural network or something, then you wouldn't need to know the dynamics model equations either, as you would differentiate the model parameters.

So, assuming we have a nice differentiable environment model, GB-MPC works very simply by sampling an initial trajectory of actions, running them through the dynamics model and collecting rewards, then updating the sampled actions via gradient descent.

$$
\begin{array}{l}
\hline
\textbf{Algorithm: } \text{Gradient-Based MPC (GB-MPC)} \\
\hline
\textbf{Input: } \text{Initial State } x_0, \text{ Rollout length } k, \text{ Number of Rollouts } n, \text{ Iterations } I, \text{ Learning Rate } \eta \\ 
\textbf{Initialize: } \text{Initial action sequence } \mathbf{u}_{0:k-1} \text{ (e.g., from } \mathcal{N}(0, I) \text{ or prev. step)} \\
\hline
1: \textbf{for iterations } i = 1 \dots I \textbf{ do} \\
2: \quad \textbf{for } \text{each sequence } i = 1 \dots n \textbf{ do} \\
2: \quad \quad \text{Forward pass: Roll out trajectory using } \hat{x}_{t+1} = f_\theta(\hat{x}_t, u_t) \\
3: \quad \quad \text{Compute total objective: } J^{(i)} = \sum_{j=0}^{k-1} R(\hat{x}_j, u_j) \\
4: \quad \textbf{end for} \\
5: \quad \text{Average objectives from sequences } J = \frac{1}{I}\sum_{i=1}^{I}J^{(i)} \\
4: \quad \text{Backward pass: Calculate gradients } \nabla_{\mathbf{u}} J = \frac{\partial J}{\partial \mathbf{u}_{0:k-1}} \\
5: \quad \text{Update actions: } \mathbf{u}_{0:k-1} \leftarrow \mathbf{u}_{0:k-1} + \eta \nabla_{\mathbf{u}} J \\
6: \textbf{end for} \\
7: \textbf{return } u_t^* = \mathbf{u}_0 \\
\hline
\end{array}
$$

The positive aspect of gradient-based methods is that they guide the updates of actions in the direction of improvement. This differs from population based methods which just improve the action values by aggregating the outputs of each trajectory. However, the challenge with gradient descent is that it can get caught in local optima, so it may miss the optimal solution.

## A Simple Simulation
Detail experiments...

## Reflections
What are your learnings and thoughts...