# Model Predictive Control (MPC) with black-box dynamics models

I've always found decision-making agents one of the most interesting areas of Artificial Intelligence (AI). It was this fascination which drew me to studying Reinforcement Learning (RL) in my master's thesis (link?). However, my constant frustration with playing with RL algorithms is how long it takes to get an agent to learn anything. When running models for my thesis, despite using a high performance computing cluster with A100 GPUs my quicker training runs would take 6-7 hours, and for some more complicated models I was waiting several days to get results.

After finishing my studies this issue only got worse. My personal computer is a laptop without a GPU, so training RL agents on anything but the simplest environments takes several hours. Its not just GPU and compute that causes this, RL is notoriously sample inefficient and brittle [source??]. This frustration led me to start looking into alternative ways to build decision making agents that would not require extensive time and compute. Yann LeCunn is frequently quoted as stating<a href="https://drive.google.com/file/d/1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi/view"> "Abandon Reinforcement Learning in favour of Model Predictive Control"</a>, the reasoning being that if you have a world / dynamics model, then you can use MPC to plan a series of actions without the lengthy training time required by model-free RL. [but of course - where do you get a world model??]

## Figuring out Model Predictive Control
My initial attempts at learning MPC were actually ill fated. I found that introductions were either trivial or overly focusses on specific applications. In the first case, only the general skeleton of the algorithm was described (which is quite easy to understand). In the second, the world model was described in great detail and this would distract from how the MPC algorithm _actually_ worked. MPC is a technique often used in engineering fields, so I found many descriptions often assumed a significant working knowledge of physics and differential equations. 

### The simple bit
MPC is a relatively simple algorithm that involves taking an action and predicting the reward (or cost) of that action by simulating the outcome of the action using a model of your environment. The typical approach is to select actions for k-steps into the future (a trajectory), and evaluate the total reward of that trajectory. You run several of these rollouts and pick the trajectory that achieves the best reward or lowest cost and then take the first action from that trajectory to apply in your environment. You repeat this everytime you need to make a decision, thus enabling you to choose actions which should maximise your reward over a forward window based on your knowledge of the environment.

### The more complicated bit
So far so good - but this simple algorithm has a lot of complexity to unpack:

1. Firstly - where do you get this world model?
2. Secondly - how do you choose these actions? Even if you know how the environment changes in response to an action, and the benefit of the action, its not a trivial thing to choose an action in a smart way.

With regards to the first point on the world model - many applications of MPC occur in engineering where there are well known physical models of the world, so the world model (commonly referred to as a dynamics model) doesn't have to be learnt, its just applied. But the MPC algorithm itself doesn't depend on the world model itself necessarily. So while we need an implementation of a world model to actually implement MPC, we do not necessarily need to know all of the equations.

Choosing an action is basically what MPC algorithms are designed to do. In the remainder of this post I'm going to detail four key approaches which can select actions under the assumption that you have a world / dynamics model available. I find this is an easy entry point into understanding what MPC does, and it nicely avoids all the complicated derivations for particular engineering applications which commonly pop up.

## Choosing actions in MPC
Chooing actions in MPC is essentially solving an optimisation problem: find the actions that maximise the reward (minimise the cost) over the next k-steps in the future. Lets suppose that we have an implementation of our dynamics model, but we don't know the equations. Essentially we have a black box simulator.

### 1. Random Shooting
The simplest approach is to simply randomly sample a trajectory of actions. We select a rollout length of $k$, a number of rollouts $n$, and then randomly sample $k \times n$ actions and pass each of these through our dynamics model. Take the action trajectory which achieved the best reward and take the first action from that trajectory to be applied in the environment.

$$
\begin{array}{l}
\hline
\textbf{Algorithm: } \text{Random Shooting MPC} \\
\hline
\textbf{Input: } \text{Initial State } x_0, \text{ Rollout length } k, \text{ Number of rollouts } n, \text{ Model fn } f_\theta, \text{ Reward fn } R \\
\textbf{Output: } \text{First action } u_t^* \\
\hline\
1: \text{Sample } n \text{ random sequences: } \{ \mathbf{u}^{(i)}_{0:k-1} \}_{i=1}^n \\
2: \textbf{for } \text{each sequence } i = 1 \dots n \textbf{ do} \\
3: \quad \text{Predict trajectory using } \hat{x}_{j+1} = f_\theta(\hat{x}_j, u_j) \\
4: \quad \text{Calculate total reward: } J^{(i)} = \sum_{j=0}^{k-1} R(\hat{x}_j, u_j) \\
5: \textbf{end for} \\
6: \text{Select best sequence: } i^* = \arg\max_i J^{(i)} \\
7: \textbf{return } u_t^* = \mathbf{u}^{(i^*)}_{0} \\
\hline
\end{array}
$$

Random shooting is simple to implement and quick to run, however, you might wonder how well you can select the correct actions simply by random sampling. In addition, there is no learning or refinement of actions so it is a bit inefficient in generating good actions as the only rollout that determines your reward is the one you select - but all of those other rollouts you generated contain valuable information which you could use to improve your reward!

### 2. The Cross-Entropy-Method (CEM)
CEM is a genetic algorithm or population-based method, which randomly samples actions and then refines those actions by aggregating the top-m or top-$\alpha$ percentile reward-achieving-trajectories. The algorithm works much like the random-shooting algorithm with the addition of the refinement process. As a concrete example, assume we are sampling actions from a standard (multivariate) normal distribution, i.e. $ \mathbf{a}\_{t} \sim N(\mathbf{0}\_{n}, \mathbf{I}\_{n}) $ (bold-faced to represent vector of length $n$). We then run our trajectories through our dynamics model and rank them by their achieved rewards. We select the $m$ best, or use a percentile cutoff to create a subset of trajectories referred to as "elites". Using only these elites we recalculate the entries of the mean and standard deviation of the sampling distribution from the corresponding vector position of the elites. So - to be clear - we are maintaining a normal distribution over each time-step in the rollout.

$$
\begin{array}{l}
\hline
\textbf{Algorithm: } \text{Cross-Entropy Method (CEM)} \\
\hline
\textbf{Input: } \text{Initial State } x_0, \text{ Rollout length } k, \text{ Number of Rollouts } n, \text{ Number of Elites } m, \text{ Iterations } I \\ 
\text{ Model fn } f_\theta, \text{ Reward fn } R \\
\textbf{Output: } \text{First action } u_t^* \\
\textbf{Initialize: } \mu_{j} = \mathbf{0}_n, \sigma_{j} = \mathbf{I}_n \text{ for } j = 0 \dots k-1 \\
\hline
1: \textbf{for } \text{iteration } 1 \dots I \textbf{ do} \\
2: \quad \text{Sample } n \text{ sequences: } \mathbf{u}^{(i)}_{0:k-1} \sim \mathcal{N}(\mu_{0:k-1}, \sigma_{0:k-1}) \\
3: \quad \textbf{for } \text{each sequence } i = 1 \dots n \textbf{ do} \\
4: \quad \quad \text{Predict trajectory } \hat{x}_{j+1} = f_\theta(\hat{x}_j, u_j) \\
5: \quad \quad \text{Calculate total reward: } J^{(i)} = \sum_{j=0}^{k-1} R(\hat{x}_j, u_j) \\
6: \quad \textbf{end for} \\
7: \quad \text{Select top-} m \text{  sequences with highest } J^{(i)} \\
8: \quad \text{Update } \mu_j = \text{mean}(\text{elites}_j) \text{ for } j = 0 \dots k-1 \\
9: \quad \text{Update } \sigma_j = \text{std}(\text{elites}_j) \text{ for } j = 0 \dots k-1 \\
10: \textbf{end for} \\
11: \textbf{return } u_t^* = \mu_0 \\
\hline
\end{array}
$$



### 3. Model Predictive Path Integral (MPPI)

### 4. Gradient-Based MPC

## A Simple Simulation
Detail experiments...

## Reflections
What are your learnings and thoughts...